

We will use a huge amount of data that provided by [HC Corpora](http://www.corpora.heliohost.org/). It will be used to train our app to predict next word in the text that user types. The data was collected by **HC Corpora** and recompiled for **Coursera** students by **Swiftkey** team. This package contains sets of News, Blog Posts and Tweets with 4 languages - English, Deutch, Finnish and Russian. I will use English texts for our learning purposes. 

Before we start let's ensure we have the data and load it. I will use speacially prepared utils to load it into varaibles with names *twitter.lines*, *blogs.lines*  and *news.lines*.

```{r results='hide', warning=FALSE, message=FALSE}
source('utils/requirements.R')
source('utils/data.R')
prepare_data()
```

Function *prepare_data()* will ensure that **Coursera-SwiftKey.zip** is downloaded and that necessery files are unzipped into *data* folder. 

## Examine files 

I will check the line qty and it's statistical properties for every file. I will also try to find more interesting facts about the data. 

Let's start from file sizes and the qty if line for every file. 

```{r, cache=TRUE, warning=FALSE}
get_file_by_name = function(name){
    paste(c('en_US', name, 'txt'), collapse = ".")
}

get_file_path_by_name = function(name){
    file_name = get_file_by_name(name)
    file_path = paste(c(data_folder_name, file_name), collapse = "/")    
}

get_data = function(name){
    file_path = get_file_path_by_name(name)
    scan(file_path, what = "character", sep = "\n")
}

# Sample size is 25%
sample_size = 0.25

twitter.lines <- get_data('twitter')
twitter.lines.lengths = nchar(twitter.lines)
twitter.lines.summary = summary(twitter.lines.lengths)
twitter.lines.sample = twitter.lines[
    sample(
        1:length(twitter.lines), 
        round(length(twitter.lines) * sample_size)
    )
]
rm(twitter.lines)

blogs.lines <- get_data('blogs')
blogs.lines.lengths = nchar(blogs.lines)
blogs.lines.summary = summary(blogs.lines.lengths)
blogs.lines.sample = blogs.lines[
    sample(
        1:length(blogs.lines), 
        round(length(blogs.lines) * sample_size)
    )
]
rm(blogs.lines)

news.lines <- get_data('news')
news.lines.lengths = nchar(news.lines)
news.lines.summary = summary(news.lines.lengths)
news.lines.sample = news.lines[
    sample(
        1:length(news.lines), 
        round(length(news.lines) * sample_size)
    )
]
rm(news.lines)

get_file_size_in_mb = function(name){
    file_path = get_file_path_by_name(name)
    file_info = file.info(file_path)
    file_info$size / 1024.0 / 1024.0
}

get_line_qty = function(variable){
    length(variable)
}

row.names = c('Tweets', 'Blog Posts', 'News Records')
row.file_names = c(get_file_by_name('twitter'), get_file_by_name('blogs'), get_file_by_name('news'))
row.file_sizes = c(get_file_size_in_mb('twitter'), get_file_size_in_mb('blogs'), get_file_size_in_mb('news'))
row.line_qtys = c(
    get_line_qty(twitter.lines.lengths), 
    get_line_qty(blogs.lines.lengths), 
    get_line_qty(news.lines.lengths)
)

row.min_lines = c(twitter.lines.summary[1], blogs.lines.summary[1], news.lines.summary[1])
row.max_lines = c(twitter.lines.summary[6], blogs.lines.summary[6], news.lines.summary[6])
row.mean_lines = c(twitter.lines.summary[4], blogs.lines.summary[4], news.lines.summary[4])
row.median_lines = c(twitter.lines.summary[3], blogs.lines.summary[3], news.lines.summary[3])

summary.table = data.frame(
    row.names, 
    row.file_names, 
    row.file_sizes, 
    row.line_qtys,
    row.min_lines,
    row.max_lines,
    row.mean_lines,
    row.median_lines
)

colnames(summary.table) = c(
    "Data Source", 
    "File Name",
    "File Size, Mb",
    "Lines in the file",
    "Min Line Lenght",
    "Max Line Lenght",
    "Avg Line Lenght",
    "Median of Line Lenght"
)
```

We've prepared data. So let's print it. 

```{r}
pander(summary.table, justify = 'left')
```

To understand the dencity of the lenghs for each of data source let's build violin plot. It will show us the real picture about length of lines in the corpus. 

```{r cache=TRUE}
twitter.df = data.frame(
    twitter.lines.lengths, 
    rep("Twitter", length(twitter.lines.lengths))
)
colnames(twitter.df) = c('length', 'source') 

blogs.df = data.frame(
    blogs.lines.lengths, 
    rep("Blogs", length(blogs.lines.lengths))
)
colnames(blogs.df) = c('length', 'source') 

news.df = data.frame(
    news.lines.lengths, 
    rep("News", length(news.lines.lengths))
)
colnames(news.df) = c('length', 'source') 

combined.df = rbind(twitter.df, blogs.df, news.df)
```

We can see by **Median of Line Length** that it's mostly below *200* and the max length of line is `r max(row.max_lines)`. So I will limit the **lengths** axis to make it more clear. And will add median mark 

```{r warning=FALSE, out.width=800}
ggplot(combined.df, aes(x=factor(source), y=length)) + 
    geom_violin() +
    geom_boxplot(width=.1, fill="black", outlier.colour=NA) +
    stat_summary(fun.y=median, geom="point", fill="white", shape=21, size=2.5) +
    ylim(0, 1000)
```

What can we resume? That the **Tweets**, **Blog Posts** and **News Lines** has mostly same size. But **Blog Posts** and **News** has much more higther maximum length of lines.

## Keyword Dencity Analysis

To realize the content of the gived sources I'd like clean it up and tokenize it with n-grams. But as soon as the original data source is huge I need to prepare the sample data set that is smaller. And it's easier to work with. 

### Samplpling the Data

I will prepare the sample that cantains 10% of all provided data. For that let's use 10% samples I've prepared before. 

```{r}
sample.lines  = c(twitter.lines.sample, blogs.lines.sample, news.lines.sample)
rm(twitter.lines.sample)
rm(blogs.lines.sample)
rm(news.lines.sample)
```

Now I have sample data set that contains `r length(sample.lines)` lines with representative data. We can easly process it but in the other hand we can make some suggestions on the basis of that data. 

### Clean Up Sample Data Set

On this step I will create new *Corpus* of the **tm** package and remove from text stop words, numbers and punctuation. I also would like to remove profanity words from all texts to prevent my work from being confusing. I will use following list as profanity dictionary - [https://gist.github.com/ryanlewis/a37739d710ccdb4b406d](https://gist.github.com/ryanlewis/a37739d710ccdb4b406d).

I've examined some control characters in the corpus. As first step of cleanup process I need to remove them from corpus.

```{r cache=TRUE}
sample.lines <- iconv(sample.lines, to="UTF-8", sub="byte")
```

We need to find per senrtence n-grams. Our main goal is to predict next word of text on per sentence basis. So, the next step is te split lines into sentences. 

```{r cache=TRUE}
split_to_sentences = function(lines){
    unlist(lines %>% regexp_tokenizer(pattern = '[\\.\\?\\!]+'))    
}

sample.sentences <- split_to_sentences(sample.lines)
rm(sample.lines)
```

### Tockenize Cleaned Sample Corpus

Ok, the data is clear and I can tokenize it using 1,2,3 and 4-grams. After that I will define more recent used words and build wordcloud for these sets of words. 

I will tokenize corpus and find n-grams using **text2vec** package. I found is a fastest way to get job done. 

```{r cache=TRUE}

# Load stopwords from external dictionary
words_to_remove = c(
    get_profanity_words(), 
    tokenizers::stopwords('en')
)

# Prepare tokenizer function
get_tokens = function(sentences){
    tokens = itoken(
        sentences, 
        preprocess_function = tolower, 
        tokenizer = tokenizers::tokenize_words, 
        chunks_number = 10, 
        progessbar = TRUE
    )
    tokens
}

# Extract n-grams from tokens vocabluary
prepare_ngrams = function(lines, ngrams_num){
    tokens <- get_tokens(lines)
    min_n_gram <- ngrams_num
    max_n_gram <- ngrams_num
    vocab = create_vocabulary(
        tokens, 
        c(min_n_gram, max_n_gram), 
        stopwords <- words_to_remove
    )
    vocab <- prune_vocabulary(vocab, term_count_min = 2)
    vocab$vocab
}

get_ngrams <- function(lines, ngram_count, limit = FALSE, remove_underscore = FALSE){
    vocabluary <- prepare_ngrams(lines, ngram_count)
    ngrams <- select(vocabluary, terms, terms_counts) %>% arrange(desc(terms_counts))
    
    if (limit){
        ngrams <- ngrams[1:limit, ]
    }
    
    if (remove_underscore){
        ngrams$terms <- gsub("\\_", " ", ngrams$terms)
    }
    
    colnames(ngrams) <- c("Terms", "Count")
    ngrams
}

unigrams <- get_ngrams(sample.sentences, 1, 50, remove_underscore = TRUE)
bigrams <- get_ngrams(sample.sentences, 2, 50, remove_underscore = TRUE)
trigrams <- get_ngrams(sample.sentences, 3, 50, remove_underscore = TRUE)
```

After we've prepared *n-grams* we can plot it on wordcloud. I'd like to use wordcloud because it helps to understand how word dencity looks like. 

#### Top 50 Unigrams 

```{r out.width=800, warning=FALSE}
set.seed(12345)
pal <- brewer.pal(6, "Dark2")
wordcloud(
  words = unigrams$Terms, 
  freq = unigrams$Count,
  random.order=FALSE, 
  use.r.layout=FALSE,
  colors = pal
)
```

#### Top 50 Bigrams 

```{r out.width=800, warning=FALSE}
set.seed(12345)
pal <- brewer.pal(6, "Dark2")
wordcloud(
  words = bigrams$Terms, 
  freq = bigrams$Count,
  random.order=FALSE, 
  use.r.layout=FALSE,
  colors = pal
)
```

#### Top 50 Trigrams 

```{r out.width=800, warning=FALSE}
set.seed(12345)
pal <- brewer.pal(6, "Dark2")
wordcloud(
  words = trigrams$Terms, 
  freq = trigrams$Count,
  random.order=FALSE, 
  use.r.layout=FALSE,
  colors = pal
)
```

Looks great! We can see top 50 of *n-grams* which used in corpus. So we can realize what *n-grams* most recently used in the texts. Sure, we discovered sample only. But in statistical matter the sample completely represents it's source. 

### Conclusions

+ We can see that the most of all texts in data sets are not so long. Mostly it's length is from 0 to 250 characters. And it contains not many sentences. So we can reduce our work and use our clean up model for prediction purposes. 
+ The most used words are really looks like widely used. So we have chance to build really working prediction model if we will use *n-grams* as the base for our prediction.
+ The data we work with is a bit huge and it consume a lot of memory. First of all I will try to use corpus DB on cleanup stage. And in the other hand I will cache the results of calculation into RDS files. That will help me to deploy the results of works to publicity. 
+ I've found that using of multiprocessing is great if word with huge data. I've used *mc.cores* option with *tm_map()* and will try to find ways to use similar techincue on prediction stage. As soon as really increase the performance of the alghorithm.

Also there are some conclustions that can help to make prediction model better. 

+ There are some abbreviations we should convert into full form on cleaning up stage. For example *I'm* should be converted into *I am* etc. We will have more consistant data due to this. 
+ I think we shouldn't remove stop-words according to purposes of this work, because we build prediction model which helps humans to type text. And the stopwords is the important part of the natural language. At least I will try to test prediction models with and without stopwords to test which will works better. 

P.S. Then I started the *NGramTokenizer* on 10% sample I've got odd memory linked with *Out Of Java Memory*. The cutting vector into pieces helped me but I'm looking to *quanteda* package to use it in my later works. 















